# Training an LLM from Scratch

Sam Foreman  
2024-02-13

We will be using [`wordplay`](https://github.com/saforem2/wordplay) to train an
LLM from scratch.

Specifically, we will walk through the
[`shakespeare.ipynb`](https://github.com/saforem2/wordplay/blob/main/notebooks/shakespeare.ipynb)
notebook which builds and trains a small (~10M param) model on the Shakespeare dataset.

- slides: [\[Online\]](https://saforem2.github.io/llm-workshop-talk/#/title-slide) [\[GitHub\]](https://github.com/saforem2/llm-workshop-talk)

- project: [\[ `wordplay` ðŸŽ®ðŸ’¬ \]](https://github.com/saforem2/wordplay)  [\[Online\]](https://saforem2.github.io/wordplay)

- notebook: [\[`shakespeare.ipynb`\]](https://github.com/saforem2/wordplay/blob/main/notebooks/shakespeare.ipynb) [\[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\]](https://colab.research.google.com/github/saforem2/wordplay/blob/main/notebooks/shakespeare.ipynb)
